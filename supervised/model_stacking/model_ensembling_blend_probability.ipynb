{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Ensembling\n",
    "Blog Reference: https://mlwave.com/kaggle-ensembling-guide/\n",
    "\n",
    "Github Referemce: https://github.com/MLWave/Kaggle-Ensemble-Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-d341fc58bb10>, line 55)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-d341fc58bb10>\"\u001b[0;36m, line \u001b[0;32m55\u001b[0m\n\u001b[0;31m    id = md5.new(\"{}\"{}tr(clf.get_params())).hexdigest()\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import md5\n",
    "import json\n",
    "\n",
    "def blend_proba(clf, X_train, y, X_test, nfolds=5, save_preds=\"\",\n",
    "                save_test_only=\"\", seed=300373, save_params=\"\",\n",
    "                clf_name=\"XX\", generalizers_params=[], minimal_loss=0,\n",
    "                return_score=False, minimizer=\"log_loss\"):\n",
    "  print(\"\\nBlending with classifier:\\n\\t{}\".format(clf))\n",
    "  folds = list(cross_validation.StratifiedKFold(y, nfolds,shuffle=True,random_state=seed))\n",
    "  print(X_train.shape)\n",
    "  dataset_blend_train = np.zeros((X_train.shape[0],np.unique(y).shape[0]))\n",
    "\n",
    "  #iterate through train set and train - predict folds\n",
    "  loss = 0\n",
    "  for i, (train_index, test_index) in enumerate( folds ):\n",
    "    print(\"Train Fold {}/{}}\".format(i+1,nfolds))\n",
    "    fold_X_train = X_train[train_index]\n",
    "    fold_y_train = y[train_index]\n",
    "    fold_X_test = X_train[test_index]\n",
    "    fold_y_test = y[test_index]\n",
    "    clf.fit(fold_X_train, fold_y_train)\n",
    "\n",
    "    fold_preds = clf.predict_proba(fold_X_test)\n",
    "    print(\"Logistic loss: {}\".format(log_loss(fold_y_test,fold_preds)))\n",
    "    dataset_blend_train[test_index] = fold_preds\n",
    "    if minimizer == \"log_loss\":\n",
    "      loss += log_loss(fold_y_test,fold_preds)\n",
    "    if minimizer == \"accuracy\":\n",
    "      fold_preds_a = np.argmax(fold_preds, axis=1)\n",
    "      loss += accuracy_score(fold_y_test,fold_preds_a)\n",
    "    #fold_preds = clf.predict(fold_X_test)\n",
    "\n",
    "    #loss += accuracy_score(fold_y_test,fold_preds)\n",
    "\n",
    "    if minimal_loss > 0 and loss > minimal_loss and i == 0:\n",
    "      return False, False\n",
    "    fold_preds = np.argmax(fold_preds, axis=1)\n",
    "    print(\"Accuracy:      {}\".format(accuracy_score(fold_y_test,fold_preds)))\n",
    "  avg_loss = loss / float(i+1)\n",
    "  print(\"\\nAverage:\\t{}\\n\".format(avg_loss))\n",
    "  #predict test set (better to take average on all folds, but this is quicker)\n",
    "  print(\"Test Fold 1/1\")\n",
    "  clf.fit(X_train, y)\n",
    "  dataset_blend_test = clf.predict_proba(X_test)\n",
    "\n",
    "  if clf_name == \"XX\":\n",
    "    clf_name = str(clf)[1:3]\n",
    "\n",
    "  if len(save_preds)>0:\n",
    "    id = md5.new(\"{}\"{}tr(clf.get_params())).hexdigest()\n",
    "    print(\"storing meta predictions at: {}\"{}ave_preds)\n",
    "    np.save(\"{}_{}_{}_train.npy\".format((save_preds,clf_name,avg_loss,id),dataset_blend_train))\n",
    "    np.save(\"{}_{}_{}_test.npy\".format((save_preds,clf_name,avg_loss,id),dataset_blend_test))\n",
    "\n",
    "  if len(save_test_only)>0:\n",
    "    id = md5.new(\"{}\"{}tr(clf.get_params())).hexdigest()\n",
    "    print(\"storing meta predictions at: {}\"{}ave_test_only)\n",
    "\n",
    "    dataset_blend_test = clf.predict(X_test)\n",
    "    np.savetxt(\"{}_{}_{}_test.txt\".format((save_test_only,clf_name,avg_loss,id),dataset_blend_test))\n",
    "    d = {}\n",
    "    d[\"stacker\"] = clf.get_params()\n",
    "    d[\"generalizers\"] = generalizers_params\n",
    "    with open(\"{}_{}_{}_params.json\".format((save_test_only,clf_name,avg_loss, id), 'wb')) as f:\n",
    "      json.dump(d, f)\n",
    "\n",
    "  if len(save_params)>0:\n",
    "    id = md5.new(\"{}\"{}tr(clf.get_params())).hexdigest()\n",
    "    d = {}\n",
    "    d[\"name\"] = clf_name\n",
    "    d[\"params\"] = { k:(v.get_params() if \"\\n\" in str(v) or \"<\" in str(v) else v) for k,v in clf.get_params().items()}\n",
    "    d[\"generalizers\"] = generalizers_params\n",
    "    with open(\"{}_{}_{}_params.json\".format((save_params,clf_name,avg_loss, id), 'wb')) as f:\n",
    "      json.dump(d, f)\n",
    "\n",
    "  if np.unique(y).shape[0] == 2: # when binary classification only return positive class proba\n",
    "    if return_score:\n",
    "      return dataset_blend_train[:,1], dataset_blend_test[:,1], avg_loss\n",
    "    else:\n",
    "      return dataset_blend_train[:,1], dataset_blend_test[:,1]\n",
    "  else:\n",
    "    if return_score:\n",
    "      return dataset_blend_train, dataset_blend_test, avg_loss\n",
    "    else:\n",
    "      return dataset_blend_train, dataset_blend_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
